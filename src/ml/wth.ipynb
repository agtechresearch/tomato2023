{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n",
      "/usr/share/python-wheels/urllib3-1.25.8-py2.py3-none-any.whl/urllib3/connectionpool.py:1004: InsecureRequestWarning: Unverified HTTPS request is being made to host 'pypi.ngc.nvidia.com'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/latest/advanced-usage.html#ssl-warnings\n"
     ]
    }
   ],
   "source": [
    "!pip install -q yfinance gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from collections import deque\n",
    "import yfinance as yf\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "class NasdaqMarketEnv(gym.Env):\n",
    "    def __init__(self, ticker, initial_cash=10000, start_date='2010-01-01', end_date='2023-12-31'):\n",
    "        super(NasdaqMarketEnv, self).__init__()\n",
    "        self.ticker = ticker\n",
    "        self.start_date = start_date\n",
    "        self.end_date = end_date\n",
    "        self.initial_cash = initial_cash\n",
    "        self.data = self._load_data()\n",
    "        self.action_space = spaces.Discrete(2)  # Buy, Sell\n",
    "        self.observation_space = spaces.Box(low=0, high=np.inf, shape=(4,))\n",
    "        self.reset()\n",
    "\n",
    "    def _load_data(self):\n",
    "        stock_data = yf.download(self.ticker, start=self.start_date, end=self.end_date)\n",
    "        return stock_data['Close'].values\n",
    "\n",
    "    def reset(self):\n",
    "        # Reset environment to initial state\n",
    "        self.current_step = 0\n",
    "        self.cash = self.initial_cash\n",
    "        self.shares = 0\n",
    "        self.portfolio_value = self.initial_cash\n",
    "        return self._get_observation()\n",
    "\n",
    "    def step(self, action):\n",
    "        # Execute one time step within the environment\n",
    "        reward = 0\n",
    "        done = False\n",
    "\n",
    "        # Execute action\n",
    "        if action == 0:  # Buy\n",
    "            if self.cash > self.data[self.current_step]:\n",
    "                self.shares += 1\n",
    "                self.cash -= self.data[self.current_step]\n",
    "            else:\n",
    "                reward -= 1  # Penalize for invalid action\n",
    "        elif action == 1:  # Sell\n",
    "            if self.shares > 0:\n",
    "                self.shares -= 1\n",
    "                self.cash += self.data[self.current_step]\n",
    "            else:\n",
    "                reward -= 1  # Penalize for invalid action\n",
    "\n",
    "        # Move to the next time step\n",
    "        self.current_step += 1\n",
    "\n",
    "        # Calculate portfolio value\n",
    "        self.portfolio_value = self.cash + self.shares * self.data[self.current_step]\n",
    "\n",
    "        # Check if done\n",
    "        if self.current_step >= len(self.data) - 1:\n",
    "            done = True\n",
    "\n",
    "        # Calculate reward (simple return)\n",
    "        reward += (self.portfolio_value - self.initial_cash) / self.initial_cash\n",
    "\n",
    "        return self._get_observation(), reward, done, {}\n",
    "\n",
    "    def _get_observation(self):\n",
    "        # Get the observation\n",
    "        return np.array([self.cash, self.shares, self.portfolio_value, self.data[self.current_step]])\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    def __init__(self, state_size, action_size):\n",
    "        super(QNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_size, 24)\n",
    "        self.fc2 = nn.Linear(24, 24)\n",
    "        self.fc3 = nn.Linear(24, action_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)\n",
    "\n",
    "class DQNAgent:\n",
    "    def __init__(self, state_size, action_size, learning_rate=0.001, gamma=0.95, epsilon=1.0, epsilon_min=0.01, epsilon_decay=0.995):\n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=2000)\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.epsilon_min = epsilon_min\n",
    "        self.epsilon_decay = epsilon_decay\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        self.model = QNetwork(state_size, action_size).to(self.device)\n",
    "        self.optimizer = optim.Adam(self.model.parameters(), lr=learning_rate)\n",
    "        self.loss = []\n",
    "\n",
    "    def remember(self, state, action, reward, next_state, done):\n",
    "        # Store the experience in memory\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def act(self, state):\n",
    "        # Epsilon-greedy action selection\n",
    "        if np.random.rand() <= self.epsilon:\n",
    "            return random.randrange(self.action_size)\n",
    "        else:\n",
    "            with torch.no_grad():\n",
    "                state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "                q_values = self.model(state)\n",
    "                return q_values.max(1)[1].item()\n",
    "\n",
    "    def replay(self, batch_size):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "        \n",
    "        # Sample minibatch from the memory\n",
    "        minibatch = random.sample(self.memory, batch_size)\n",
    "        \n",
    "        for state, action, reward, next_state, done in minibatch:\n",
    "            state = torch.FloatTensor(state).unsqueeze(0).to(self.device)\n",
    "            next_state = torch.FloatTensor(next_state).unsqueeze(0).to(self.device)\n",
    "            target = reward\n",
    "            if not done:\n",
    "                target = reward + self.gamma * torch.max(self.model(next_state)[0])\n",
    "            \n",
    "            q_values = self.model(state)\n",
    "            old_q_value = q_values[0][action]\n",
    "            q_values[0][action] = target\n",
    "            loss = F.mse_loss(q_values, self.model(state))\n",
    "            \n",
    "            # Optimize the model\n",
    "            self.optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            self.loss.append(abs(loss.item() - old_q_value.item()))\n",
    "\n",
    "        if self.epsilon > self.epsilon_min:\n",
    "            self.epsilon *= self.epsilon_decay\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[*********************100%%**********************]  1 of 1 completed"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode: 1/100, Loss: 25.16925684812544, Last Reward: -0.87581484375\n",
      "Episode: 2/100, Loss: 1.903290557432102, Last Reward: -1.0524020751953125\n",
      "Episode: 3/100, Loss: 5.156110075053837, Last Reward: -1.0032190185546874\n",
      "Episode: 4/100, Loss: 7.918836825633438, Last Reward: -0.9467000244140625\n",
      "Episode: 5/100, Loss: 9.719792309220042, Last Reward: -0.8752760498046875\n",
      "Episode: 6/100, Loss: 13.919986788634898, Last Reward: -0.8633280029296875\n",
      "Episode: 7/100, Loss: 17.19683413287164, Last Reward: -0.766784033203125\n",
      "Episode: 8/100, Loss: 17.46519942345452, Last Reward: -1.0400640380859376\n",
      "Episode: 9/100, Loss: 16.918044972061608, Last Reward: -0.773969091796875\n",
      "Episode: 10/100, Loss: 15.426840573503245, Last Reward: 0.1772359619140625\n",
      "Episode: 11/100, Loss: 13.373699585697949, Last Reward: 0.1458140869140625\n",
      "Episode: 12/100, Loss: 10.931734827897744, Last Reward: -0.903178271484375\n",
      "Episode: 13/100, Loss: 8.357315390813106, Last Reward: 0.1594358642578125\n",
      "Episode: 14/100, Loss: 6.476055632636213, Last Reward: 0.0415530029296875\n",
      "Episode: 15/100, Loss: 5.345730722627475, Last Reward: -1.0585779052734374\n",
      "Episode: 16/100, Loss: 4.745717330687239, Last Reward: 0.1915111328125\n",
      "Episode: 17/100, Loss: 4.568797091520766, Last Reward: -0.9859249755859375\n",
      "Episode: 18/100, Loss: 5.088439125551915, Last Reward: -0.8798400146484375\n",
      "Episode: 19/100, Loss: 6.209546090835763, Last Reward: -0.996925\n",
      "Episode: 20/100, Loss: 8.655003210745283, Last Reward: -0.9989520263671875\n",
      "Episode: 21/100, Loss: 12.810650817759972, Last Reward: -1.0082650146484375\n",
      "Episode: 22/100, Loss: 16.9862194133984, Last Reward: -0.8275880126953126\n",
      "Episode: 23/100, Loss: 17.087575683626927, Last Reward: -0.87669501953125\n",
      "Episode: 24/100, Loss: 17.31877343802434, Last Reward: -0.878181005859375\n",
      "Episode: 25/100, Loss: 17.62516766324138, Last Reward: 0.1425889404296875\n",
      "Episode: 26/100, Loss: 17.255570957601428, Last Reward: 0.06950185546875\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 29\u001b[0m\n\u001b[1;32m     27\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m done:\n\u001b[1;32m     28\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[0;32m---> 29\u001b[0m         \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreplay\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     30\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpisode: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;250m \u001b[39m\u001b[38;5;241m+\u001b[39m\u001b[38;5;250m \u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepisodes\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnp\u001b[38;5;241m.\u001b[39mmean(agent\u001b[38;5;241m.\u001b[39mloss)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Last Reward: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreward\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;66;03m# 23분에 13th episode...\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[6], line 135\u001b[0m, in \u001b[0;36mDQNAgent.replay\u001b[0;34m(self, batch_size)\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[38;5;66;03m# Optimize the model\u001b[39;00m\n\u001b[1;32m    134\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 135\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    136\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloss\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mabs\u001b[39m(loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;241m-\u001b[39m old_q_value\u001b[38;5;241m.\u001b[39mitem()))\n",
      "File \u001b[0;32m~/문서/tomato2023/lib/python3.8/site-packages/torch/_tensor.py:492\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    482\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    483\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    484\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    485\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    490\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    491\u001b[0m     )\n\u001b[0;32m--> 492\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    493\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    494\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/문서/tomato2023/lib/python3.8/site-packages/torch/autograd/__init__.py:251\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    246\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 251\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    252\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    253\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    254\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    257\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    258\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    259\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "ticker = '^IXIC'  # Nasdaq index ticker symbol\n",
    "start_date = '2010-01-01'\n",
    "end_date = '2023-12-31'\n",
    "\n",
    "env = NasdaqMarketEnv(ticker, start_date=start_date, end_date=end_date)\n",
    "state_size = env.observation_space.shape[0]\n",
    "action_size = env.action_space.n\n",
    "agent = DQNAgent(state_size, action_size)\n",
    "# agent.model.to(device)\n",
    "\n",
    "batch_size = 256\n",
    "episodes = 100\n",
    "\n",
    "# 2시간에 43th episode...\n",
    "for e in range(episodes):\n",
    "    state = env.reset()\n",
    "    agent.loss = []\n",
    "    for time in range(500):\n",
    "        \n",
    "        action = agent.act(state)\n",
    "        next_state, reward, done, _ = env.step(action)\n",
    "        reward = reward if not done else -10\n",
    "        agent.remember(state, action, reward, next_state, done)\n",
    "        state = next_state\n",
    "        # print(e, time, action, reward)\n",
    "        if done:\n",
    "            break\n",
    "        agent.replay(batch_size)\n",
    "    print(f\"Episode: {e + 1}/{episodes}, Loss: {np.mean(agent.loss)}, Last Reward: {reward}\")\n",
    "\n",
    "# batch_size 128에서는 23분에 13th episode...\n",
    "# batch_size 256에서는 12분에 4th episode...\n",
    "# batch_size 256에서는 91분에 27th episode..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tomato2023",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
